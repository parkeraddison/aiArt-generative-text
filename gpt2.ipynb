{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: gpt-2-simple in /home/pgaddiso/.local/lib/python3.7/site-packages (0.6)\n",
      "Requirement already satisfied: regex in /home/pgaddiso/.local/lib/python3.7/site-packages (from gpt-2-simple) (2017.4.5)\n",
      "Requirement already satisfied: requests in /home/pgaddiso/.local/lib/python3.7/site-packages (from gpt-2-simple) (2.21.0)\n",
      "Requirement already satisfied: tqdm in /home/pgaddiso/.local/lib/python3.7/site-packages (from gpt-2-simple) (4.31.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from gpt-2-simple) (1.16.4)\n",
      "Requirement already satisfied: toposort in /home/pgaddiso/.local/lib/python3.7/site-packages (from gpt-2-simple) (1.5)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->gpt-2-simple) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->gpt-2-simple) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/pgaddiso/.local/lib/python3.7/site-packages (from requests->gpt-2-simple) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->gpt-2-simple) (2019.9.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user gpt-2-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 298Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 38.3Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 566Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:03, 151Mit/s]                                   \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 385Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 55.4Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 72.2Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "model_name = \"124M\"\n",
    "gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1018 12:18:40.435493 140017285318464 deprecation.py:323] From /home/pgaddiso/.local/lib/python3.7/site-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1018 12:18:52.043003 140017285318464 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 21214 tokens\n",
      "Training...\n",
      "[1 | 7.15] loss=3.50 avg=3.50\n",
      "[2 | 8.40] loss=3.48 avg=3.49\n",
      "[3 | 9.68] loss=3.31 avg=3.43\n",
      "[4 | 10.93] loss=3.38 avg=3.42\n",
      "[5 | 12.20] loss=3.24 avg=3.38\n",
      "[6 | 13.46] loss=3.24 avg=3.36\n",
      "[7 | 14.72] loss=3.38 avg=3.36\n",
      "[8 | 15.98] loss=3.53 avg=3.38\n",
      "[9 | 17.23] loss=3.40 avg=3.38\n",
      "[10 | 18.48] loss=2.56 avg=3.30\n",
      "[11 | 19.73] loss=3.22 avg=3.29\n",
      "[12 | 20.99] loss=3.16 avg=3.28\n",
      "[13 | 22.25] loss=2.91 avg=3.25\n",
      "[14 | 23.51] loss=2.74 avg=3.21\n",
      "[15 | 24.77] loss=3.29 avg=3.22\n",
      "[16 | 26.02] loss=2.66 avg=3.18\n",
      "[17 | 27.28] loss=2.77 avg=3.15\n",
      "[18 | 28.54] loss=2.80 avg=3.13\n",
      "[19 | 29.79] loss=2.87 avg=3.12\n",
      "[20 | 31.05] loss=2.23 avg=3.07\n",
      "[21 | 32.30] loss=2.43 avg=3.03\n",
      "[22 | 33.56] loss=2.30 avg=3.00\n",
      "[23 | 34.82] loss=2.24 avg=2.96\n",
      "[24 | 36.08] loss=2.40 avg=2.93\n",
      "[25 | 37.35] loss=2.32 avg=2.91\n",
      "[26 | 38.61] loss=1.96 avg=2.86\n",
      "[27 | 39.87] loss=2.28 avg=2.84\n",
      "[28 | 41.12] loss=2.09 avg=2.81\n",
      "[29 | 42.39] loss=1.92 avg=2.78\n",
      "[30 | 43.65] loss=2.39 avg=2.76\n",
      "[31 | 44.91] loss=1.80 avg=2.72\n",
      "[32 | 46.17] loss=1.83 avg=2.69\n",
      "[33 | 47.43] loss=1.66 avg=2.66\n",
      "[34 | 48.69] loss=1.63 avg=2.62\n",
      "[35 | 49.95] loss=1.93 avg=2.60\n",
      "[36 | 51.21] loss=1.92 avg=2.57\n",
      "[37 | 52.46] loss=1.56 avg=2.54\n",
      "[38 | 53.72] loss=1.93 avg=2.52\n",
      "[39 | 54.98] loss=1.88 avg=2.50\n",
      "[40 | 56.25] loss=1.73 avg=2.48\n",
      "[41 | 57.51] loss=1.51 avg=2.45\n",
      "[42 | 58.77] loss=1.24 avg=2.42\n",
      "[43 | 60.02] loss=1.49 avg=2.39\n",
      "[44 | 61.28] loss=1.27 avg=2.36\n",
      "[45 | 62.54] loss=1.38 avg=2.33\n",
      "[46 | 63.81] loss=1.19 avg=2.30\n",
      "[47 | 65.07] loss=1.55 avg=2.28\n",
      "[48 | 66.33] loss=1.03 avg=2.25\n",
      "[49 | 67.59] loss=0.91 avg=2.21\n",
      "[50 | 68.85] loss=0.95 avg=2.18\n",
      "[51 | 70.10] loss=0.64 avg=2.14\n",
      "[52 | 71.36] loss=1.43 avg=2.13\n",
      "[53 | 72.62] loss=1.02 avg=2.10\n",
      "[54 | 73.89] loss=0.88 avg=2.07\n",
      "[55 | 75.15] loss=0.94 avg=2.04\n",
      "[56 | 76.42] loss=0.72 avg=2.01\n",
      "[57 | 77.69] loss=0.83 avg=1.99\n",
      "[58 | 78.95] loss=0.81 avg=1.96\n",
      "[59 | 80.21] loss=0.67 avg=1.93\n",
      "[60 | 81.48] loss=0.42 avg=1.90\n",
      "[61 | 82.74] loss=0.66 avg=1.87\n",
      "[62 | 84.00] loss=0.72 avg=1.84\n",
      "[63 | 85.26] loss=0.64 avg=1.82\n",
      "[64 | 86.53] loss=0.41 avg=1.79\n",
      "[65 | 87.79] loss=0.50 avg=1.76\n",
      "[66 | 89.06] loss=0.58 avg=1.74\n",
      "[67 | 90.32] loss=0.56 avg=1.71\n",
      "[68 | 91.59] loss=0.41 avg=1.69\n",
      "[69 | 92.85] loss=0.42 avg=1.66\n",
      "[70 | 94.11] loss=0.42 avg=1.64\n",
      "interrupted\n",
      "Saving checkpoint/run1/model-70\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(\n",
    "    sess,\n",
    "    \"text/w_acad_2000.txt\",\n",
    "    model_name=model_name,\n",
    "    steps=1000 # steps is max number of training steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our journey began by tackling the first core question that we were given:\n",
      "Are you more or less likely to face injury in a bike-related accident if it occurs on a bike path or off a bike path?\n",
      "To answer this, of course we needed data. The dataset we used to help us was from the CHP's Statewide Integrated Traffic Records System (SWITRS) and was accessible to use through UC Berkeley's Transportation Injury Mapping System (TIMS). Both of these are official, reputable sources, so we were confident that our data was quite reliable in terms of provenance and quality. Though, this doesn't mean it avoided all reliability concerns - we'll discuss that later.\n",
      "From TIMS, we queried bicycle collision data from 2014 through 2018 in the city of San Diego, as well as the corresponding party and victim data.\n",
      "Now that we had three tables, one of collisions, one of parties involved in each collision, and one of victims included in each party, we had to do some table joins to get all of the information in one place. We worked our way from largest to smallest scope, so our first step was to join collisions with parties. To do this, we started by identifying the columns shared by the two tables.\n",
      "Note that in Python, the & operator applied between two sets will return their intersection.\n",
      ">>> shared = list(set(collisions.columns) & set(parties.columns))\n",
      ">>> print(shared)\n",
      "['ACCIDENT_YEAR', 'CASE_ID']\n",
      "A simple inner join on these two columns gave us the merged collision and party table that we desired. In this case, we could have joined solely on CASE_ID, since the IDs did not repeat year-to-year, and ACCIDENT_YEAR provided no additional information. It's worth noting that there are multiple parties involved in a single collision. So after this merge CASE_ID is no longer a unique row value.\n",
      "To join this resulting table with victim data, we must join on more than just CASE_ID in order to avoid unwanted duplicated entries. So, we again join on all shared columns, in this case CASE_ID and PARTY_NUMBER. However, instead of conducting an inner join, we decided to use a left join. A left join will keep all rows from the left table, and will simply enter a null value if a corresponding row can't be found in the right table.\n",
      "We did this because we only had data for a subset of the victims involved. We didn't want the shortage of victim data to affect our analysis of collision data, so we wanted to keep all of the collisions, and just enter null values where victim data did not exist.\n",
      "But even with all of our tables merged, we couldn't yet answer the question at hand. We first needed to figure out which collisions occurred on a bike path or not. Although this may sound trivial, in reality a plethora of data concerns and assumptions had to be addressed in order to tackle the task.\n",
      "      Table 1. The various data issues involved in merging @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ , requiring much more data than was currently possible . Using incomplete information set ( PEMS ) to do this required a great deal of research , and was probably the hardest part of my career . But once I had a clear picture picture of which data issues were most involved , I began to identify areas in which the available research expertise was sufficient . <p> This research effort enabled me to better understand traffic accidents and injuries , and at the same time enabled me to better identify gaps in the system that could be exploited to increase its functioning and overall welfare . <p> By 1996 , the project had been successfully funded and all data collected had been used in a preliminary manner to test hypotheses about the causes of traffic accidents and injuries . <p> By 1997 , the project had been successfully successfully funded and all data collected had been used in a preliminary manner to test hypotheses about the effects of education on traffic behavior . For example , while many schools .and the general public .were cited as having little or no effect on road activity or injuries , some schoolsei effects were magnified . <p> By 1998 , the project had been successfully funded and all data collected had been used in a preliminary manner to examine the relationship between performance on a standardized test and the presence of a particular skill on a specific test . For example , while some people were noted to be highly skilled at connecting dots on a test , otherswere not . <p> By 1999 , the project had been successfully funded and all data collected had been used to examine associations between skill levels and these assessed correlations . For example , while some people were noted to be highly skilled in numeracy and writing , otherswere not . <p> By 2000 , the project had been successfully funded and all data collected had been used to examine associations between skill levels and\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    sess,\n",
    "    model_name=\"124M\",\n",
    "    temperature=0.5,\n",
    "    prefix=\"\"\"Our journey began by tackling the first core question that we were given:\n",
    "Are you more or less likely to face injury in a bike-related accident if it occurs on a bike path or off a bike path?\n",
    "To answer this, of course we needed data. The dataset we used to help us was from the CHP's Statewide Integrated Traffic Records System (SWITRS) and was accessible to use through UC Berkeley's Transportation Injury Mapping System (TIMS). Both of these are official, reputable sources, so we were confident that our data was quite reliable in terms of provenance and quality. Though, this doesn't mean it avoided all reliability concerns - we'll discuss that later.\n",
    "From TIMS, we queried bicycle collision data from 2014 through 2018 in the city of San Diego, as well as the corresponding party and victim data.\n",
    "Now that we had three tables, one of collisions, one of parties involved in each collision, and one of victims included in each party, we had to do some table joins to get all of the information in one place. We worked our way from largest to smallest scope, so our first step was to join collisions with parties. To do this, we started by identifying the columns shared by the two tables.\n",
    "Note that in Python, the & operator applied between two sets will return their intersection.\n",
    ">>> shared = list(set(collisions.columns) & set(parties.columns))\n",
    ">>> print(shared)\n",
    "['ACCIDENT_YEAR', 'CASE_ID']\n",
    "A simple inner join on these two columns gave us the merged collision and party table that we desired. In this case, we could have joined solely on CASE_ID, since the IDs did not repeat year-to-year, and ACCIDENT_YEAR provided no additional information. It's worth noting that there are multiple parties involved in a single collision. So after this merge CASE_ID is no longer a unique row value.\n",
    "To join this resulting table with victim data, we must join on more than just CASE_ID in order to avoid unwanted duplicated entries. So, we again join on all shared columns, in this case CASE_ID and PARTY_NUMBER. However, instead of conducting an inner join, we decided to use a left join. A left join will keep all rows from the left table, and will simply enter a null value if a corresponding row can't be found in the right table.\n",
    "We did this because we only had data for a subset of the victims involved. We didn't want the shortage of victim data to affect our analysis of collision data, so we wanted to keep all of the collisions, and just enter null values where victim data did not exist.\n",
    "But even with all of our tables merged, we couldn't yet answer the question at hand. We first needed to figure out which collisions occurred on a bike path or not. Although this may sound trivial, in reality a plethora of data concerns and assumptions had to be addressed in order to tackle the task.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
